{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f1936c",
   "metadata": {},
   "source": [
    "## Reading a short story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbaeea",
   "metadata": {},
   "source": [
    "### Step1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9905996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totatl characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Totatl characters:\", len(raw_text))\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a0f86",
   "metadata": {},
   "source": [
    "Our goal is to tokenize ths 20479 long text story into individual words that we can turn into embeddings for LLM training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2b555",
   "metadata": {},
   "source": [
    "Using regex split the entire book text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc95c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', ' ', 'is', ' ', 'a', ' ', 'test.', ' ', 'This', ' ', 'test', ' ', 'is', ' ', 'only', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"this is a test. This test is only a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621bce9",
   "metadata": {},
   "source": [
    "It has generated a list of individual words, whitespaces & punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717172d",
   "metadata": {},
   "source": [
    "Let's modify the regex splits on whitespaces and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8143033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', ' ', 'is', ' ', 'a', ' ', 'test', '.', '', ' ', 'This', ' ', 'test', ' ', 'is', ' ', 'only', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae29de",
   "metadata": {},
   "source": [
    "Whitespaces are still counted as token; we have to remove those and consider only words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155bc89",
   "metadata": {},
   "source": [
    "Removing whitespaces can save memory , however it can depend on the application to include or exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf28833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test', '.', 'This', 'test', 'is', 'only', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3acfc5",
   "metadata": {},
   "source": [
    "Modify the tokenization to include all the other special characters as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dbef4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test.', 'Alan', ':--', 'Do', 'you', 'like', 'it?', 'Yes,', 'I', 'do!']\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test. Alan:-- Do you like it? Yes, I do!\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da354f",
   "metadata": {},
   "source": [
    "We have a basic tokenizer and we can apply it on full book text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b3c90e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(\"Total tokens:\", len(preprocessed))\n",
    "print(preprocessed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0762d",
   "metadata": {},
   "source": [
    "#### Step2: Creating token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0626961",
   "metadata": {},
   "source": [
    "Create a list of unique tokens; Sort the tokens in alphabetical order and determine vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5c0294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Unique tokens:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d89665",
   "metadata": {},
   "source": [
    "Create the vocabulary by assigning an integer to each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db6f38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6e6370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!: 0\n",
      "\": 1\n",
      "': 2\n",
      "(: 3\n",
      "): 4\n",
      ",: 5\n",
      "--: 6\n",
      ".: 7\n",
      ":: 8\n",
      ";: 9\n",
      "?: 10\n",
      "A: 11\n",
      "Ah: 12\n",
      "Among: 13\n",
      "And: 14\n",
      "Are: 15\n",
      "Arrt: 16\n",
      "As: 17\n",
      "At: 18\n",
      "Be: 19\n",
      "Begin: 20\n"
     ]
    }
   ],
   "source": [
    "for token, integer in vocab.items():\n",
    "    print(f\"{token}: {integer}\")\n",
    "    if integer >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b713b24",
   "metadata": {},
   "source": [
    "The dictionary contains individual tokens associted with integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee8f5e",
   "metadata": {},
   "source": [
    "Note: Later in the process (output from LLM) we would need to convert the numbers back to words, so we need a way\n",
    "\n",
    "We need to create an inverse version of the vocabulary that maps token IDs back to corresponding text/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52c1ff",
   "metadata": {},
   "source": [
    "#### Step3: Implement Tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6c174",
   "metadata": {},
   "source": [
    "Let's built a tokenizer class for it.\n",
    "\n",
    "The class will have the encode method, that splits the token and carries out the string to integer mapping to produce token IDs\n",
    "\n",
    "In addition, we will implement a decode method that carries out the reverse integer to string mapping to convert token IDs to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311bf248",
   "metadata": {},
   "source": [
    "Step 1: Store the vocab as class attribute for access in encode and decode methods\n",
    "\n",
    "Step 2: Create an inverse vocab that maps token IDs back to original text token\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back to text\n",
    "\n",
    "Step 5: Replace spaces before specified pumctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbf025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # tokenize the input text using the same regex as before\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # convert tokens to integers using the vocab dictionary\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # convert integers back to tokens using the inverse vocab dictionary\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        # restore spacing around punctuation\n",
    "        text = re.sub(r'\\s([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c621955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 7, 67, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\" It's the last he painted, you know. Mrs Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "288aa902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It' s the last he painted, you know. Mrs Gisburn said with pardonable pride.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5be640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
