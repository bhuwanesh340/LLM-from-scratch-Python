{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f1936c",
   "metadata": {},
   "source": [
    "## Reading a short story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbaeea",
   "metadata": {},
   "source": [
    "### Step1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9905996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totatl characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Totatl characters:\", len(raw_text))\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a0f86",
   "metadata": {},
   "source": [
    "Our goal is to tokenize ths 20479 long text story into individual words that we can turn into embeddings for LLM training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2b555",
   "metadata": {},
   "source": [
    "Using regex split the entire book text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc95c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', ' ', 'is', ' ', 'a', ' ', 'test.', ' ', 'This', ' ', 'test', ' ', 'is', ' ', 'only', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"this is a test. This test is only a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621bce9",
   "metadata": {},
   "source": [
    "It has generated a list of individual words, whitespaces & punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717172d",
   "metadata": {},
   "source": [
    "Let's modify the regex splits on whitespaces and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8143033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', ' ', 'is', ' ', 'a', ' ', 'test', '.', '', ' ', 'This', ' ', 'test', ' ', 'is', ' ', 'only', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae29de",
   "metadata": {},
   "source": [
    "Whitespaces are still counted as token; we have to remove those and consider only words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155bc89",
   "metadata": {},
   "source": [
    "Removing whitespaces can save memory , however it can depend on the application to include or exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bf28833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test', '.', 'This', 'test', 'is', 'only', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3acfc5",
   "metadata": {},
   "source": [
    "Modify the tokenization to include all the other special characters as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dbef4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'test.', 'Alan', ':--', 'Do', 'you', 'like', 'it?', 'Yes,', 'I', 'do!']\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test. Alan:-- Do you like it? Yes, I do!\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da354f",
   "metadata": {},
   "source": [
    "We have a basic tokenizer and we can apply it on full book text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b3c90e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(\"Total tokens:\", len(preprocessed))\n",
    "print(preprocessed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0762d",
   "metadata": {},
   "source": [
    "#### Step2: Creating token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0626961",
   "metadata": {},
   "source": [
    "Create a list of unique tokens; Sort the tokens in alphabetical order and determine vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f5c0294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Unique tokens:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d89665",
   "metadata": {},
   "source": [
    "Create the vocabulary by assigning an integer to each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db6f38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6e6370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!: 0\n",
      "\": 1\n",
      "': 2\n",
      "(: 3\n",
      "): 4\n",
      ",: 5\n",
      "--: 6\n",
      ".: 7\n",
      ":: 8\n",
      ";: 9\n",
      "?: 10\n",
      "A: 11\n",
      "Ah: 12\n",
      "Among: 13\n",
      "And: 14\n",
      "Are: 15\n",
      "Arrt: 16\n",
      "As: 17\n",
      "At: 18\n",
      "Be: 19\n",
      "Begin: 20\n"
     ]
    }
   ],
   "source": [
    "for token, integer in vocab.items():\n",
    "    print(f\"{token}: {integer}\")\n",
    "    if integer >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b713b24",
   "metadata": {},
   "source": [
    "The dictionary contains individual tokens associted with integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee8f5e",
   "metadata": {},
   "source": [
    "Note: Later in the process (output from LLM) we would need to convert the numbers back to words, so we need a way\n",
    "\n",
    "We need to create an inverse version of the vocabulary that maps token IDs back to corresponding text/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52c1ff",
   "metadata": {},
   "source": [
    "#### Step3: Implement Tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6c174",
   "metadata": {},
   "source": [
    "Let's built a tokenizer class for it.\n",
    "\n",
    "The class will have the encode method, that splits the token and carries out the string to integer mapping to produce token IDs\n",
    "\n",
    "In addition, we will implement a decode method that carries out the reverse integer to string mapping to convert token IDs to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311bf248",
   "metadata": {},
   "source": [
    "Step 1: Store the vocab as class attribute for access in encode and decode methods\n",
    "\n",
    "Step 2: Create an inverse vocab that maps token IDs back to original text token\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back to text\n",
    "\n",
    "Step 5: Replace spaces before specified pumctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbf025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # tokenize the input text using the same regex as before\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # convert tokens to integers using the vocab dictionary\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # convert integers back to tokens using the inverse vocab dictionary\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        # restore spacing around punctuation\n",
    "        text = re.sub(r'\\s([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c621955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 7, 67, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\" It's the last he painted, you know. Mrs Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "288aa902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It' s the last he painted, you know. Mrs Gisburn said with pardonable pride.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f55ec3",
   "metadata": {},
   "source": [
    "The decoded method is able to decode the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc5be640",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d5a9c0ce35c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\"Hello ! there , my name is iron man.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-88e11c5f60b9>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# convert tokens to integers using the vocab dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-88e11c5f60b9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# convert tokens to integers using the vocab dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Hello ! there , my name is iron man.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19f4c9",
   "metadata": {},
   "source": [
    "Problem is \"Hello\" is not seen in the story text. Hence we are getting the error\n",
    "\n",
    "That's why it is advised to use large training corpus to extend vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d62bb7",
   "metadata": {},
   "source": [
    "### Adding Special Context Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baa2c3",
   "metadata": {},
   "source": [
    "unknown token        |unk|       --> 783\n",
    "\n",
    "end of text token    |endoftext| --> 784\n",
    "\n",
    "these token are added to handle the unknown and different data source texts effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd37180",
   "metadata": {},
   "source": [
    "We can modify the tokenizer to use |unk| token if it enconter a word that is not part of vocabulary\n",
    "\n",
    "Also we add token between unrelated text sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50fe41e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9f5d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!: 0\n",
      "\": 1\n",
      "': 2\n",
      "(: 3\n",
      "): 4\n",
      ",: 5\n",
      "--: 6\n",
      ".: 7\n",
      ":: 8\n",
      ";: 9\n",
      "?: 10\n"
     ]
    }
   ],
   "source": [
    "for token, integer in vocab.items():\n",
    "    print(f\"{token}: {integer}\")\n",
    "    if integer >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66993f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself', '<|endoftext|>', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "print(all_tokens[-10:])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6c17337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20fc8630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger: 1127\n",
      "your: 1128\n",
      "yourself: 1129\n",
      "<|endoftext|>: 1130\n",
      "<|unk|>: 1131\n"
     ]
    }
   ],
   "source": [
    "for i ,item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(f\"{item[0]}: {item[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9b62a",
   "metadata": {},
   "source": [
    "Define the v2 of Tokenizer Class\n",
    "\n",
    "Step 1: Replace unknown words by <|unk|> token\n",
    "\n",
    "Step 2: Replace spaces before the specified punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d85ecc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer:token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # tokenize the input text using the same regex as before\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        preprocessed = [token if token in self.str_to_int \n",
    "                        else \"<|unk|>\" for token in preprocessed]\n",
    "\n",
    "        # convert tokens to integers using the vocab dictionary\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # convert integers back to tokens using the inverse vocab dictionary\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        # restore spacing around punctuation\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c33af53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello do you like tea? <|endoftext|> In the sunlit terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace\"\n",
    "\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "\n",
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "237e1110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a92180fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496f110",
   "metadata": {},
   "source": [
    "For \"Hello\" and \"Palaces\" we know that there are no text in vocabulary, hence <|unk|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5626fd",
   "metadata": {},
   "source": [
    "There are few other special token used in LLM training such as:\n",
    "\n",
    "[BOS]: Beginning of sequence: This token marks start of the text.\n",
    "\n",
    "[EOS]: End of sequence: This token is positioned at the end of a text.\n",
    "\n",
    "[PAD]: Padding: When training LLM with batch size larger than one, the macth might contain texts of varying length. To ensure all texts have same length, the shorter texts are extended or \"padded\" using [PAD] token, upto lenth of longest text in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da636",
   "metadata": {},
   "source": [
    "Tokenizer used for GPT models does not need any of these tokenizers mentioned above but only uses <|endoftext|> for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9a9d1",
   "metadata": {},
   "source": [
    "The tokenizer used for GPT models doesn't use an <|unk|> token for out of vocabulary words.\n",
    "\n",
    "GPT uses as byte pair encoding tokenizer, which breaks down words into subword units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d88f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
